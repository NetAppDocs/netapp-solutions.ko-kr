---
sidebar: sidebar 
permalink: kvm/kvm-ontap.html 
keywords: netapp, kvm, libvirt, all-flash, nfs, iscsi, ontap, storage, aff 
summary: KVM 호스트의 공유 스토리지는 VM 라이브 마이그레이션 시간을 단축하고, 환경 전반의 백업 및 일관된 템플릿을 위한 더 나은 타겟을 제공합니다. ONTAP 스토리지는 KVM 호스트 환경의 요구 사항뿐만 아니라 게스트 파일, 블록 및 객체 스토리지 요구 사항도 충족할 수 있습니다. 
---
= ONTAP을 사용한 KVM 가상화
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
KVM 호스트의 공유 스토리지는 VM 라이브 마이그레이션 시간을 단축하고, 환경 전반의 백업 및 일관된 템플릿을 위한 더 나은 타겟을 제공합니다. ONTAP 스토리지는 KVM 호스트 환경의 요구 사항뿐만 아니라 게스트 파일, 블록 및 객체 스토리지 요구 사항도 충족할 수 있습니다.

KVM 호스트에는 스위치에 케이블로 연결된 FC, 이더넷 또는 기타 지원되는 인터페이스가 있어야 하며 ONTAP 논리 인터페이스와 통신할 수 있어야 합니다.

항상 https://mysupport.netapp.com/matrix/#welcome["상호 운용성 매트릭스 툴"]지원되는 구성을 확인하십시오.



== 고급 ONTAP 기능

* 공통 기능 *

* 스케일아웃 클러스터
* 보안 인증 및 RBAC 지원
* 제로 트러스트 멀티 관리자 지원
* 시큐어 멀티 테넌시
* SnapMirror을 사용하여 데이터 복제.
* 스냅샷을 사용한 시점 복제본.
* 공간 효율적인 클론.
* 중복제거, 압축 등과 같은 스토리지 효율성 기능
* Kubernetes에 대한 Trident CSI 지원
* SnapLock
* 스냅샷 복사본의 무단 잠금 방지
* 암호화 지원
* 콜드 데이터를 오브젝트 저장소에 계층화하는 FabricPool
* BlueXP와 데이터 인프라 인사이트 통합.
* Microsoft 오프로드 데이터 전송(ODX)


* NAS * 를 선택합니다

* FlexGroup 볼륨은 스케일아웃 NAS 컨테이너로, 로드 분산 및 확장성과 함께 고성능을 제공합니다.
* FlexCache를 사용하면 데이터를 전 세계에 배포할 수 있지만 데이터에 대한 로컬 읽기 및 쓰기 액세스를 제공합니다.
* 여러 프로토콜을 지원하므로 SMB 및 NFS를 통해 동일한 데이터에 액세스할 수 있습니다.
* NFS nConnect 는 TCP 연결당 여러 TCP 세션을 허용하므로 네트워크 처리량이 증가합니다. 따라서 최신 서버에서 사용할 수 있는 고속 NIC의 사용률이 증가합니다.
* NFS 세션 트렁킹은 향상된 데이터 전송 속도, 고가용성 및 내결함성 제공
* 최적화된 데이터 경로 연결을 위한 pNFS.
* SMB 다중 채널은 향상된 데이터 전송 속도, 고가용성 및 내결함성을 제공합니다.
* 파일 권한을 위해 Active Directory/LDAP와 통합
* TLS를 통해 NFS와 보안 연결
* NFS Kerberos 지원
* RDMA를 통한 NFS.
* Windows와 Unix ID 간의 이름 매핑
* 자율적 랜섬웨어 방어:
* 파일 시스템 분석:


* SAN *

* SnapMirror 활성 동기화를 사용하여 장애 도메인 간에 클러스터를 확장합니다.
* ASA 모델은 액티브/액티브 다중 경로와 빠른 경로 페일오버를 제공합니다.
* FC, iSCSI, NVMe-oF 프로토콜 지원
* iSCSI CHAP 상호 인증을 지원합니다.
* 선택적 LUN 맵 및 포트 세트.




== ONTAP 스토리지를 갖춘 Libvirt

Libvirt는 디스크 이미지 및 데이터에 NetApp ONTAP 스토리지를 활용하는 가상 머신을 관리하는 데 사용할 수 있습니다. 이러한 통합을 통해 Libvirt 기반 가상화 환경에서 데이터 보호, 스토리지 효율성, 성능 최적화 등 ONTAP의 고급 스토리지 기능을 활용할 수 있습니다. Libvirt가 ONTAP과 상호 작용하는 방식과 사용자가 수행할 수 있는 작업은 다음과 같습니다.

. 스토리지 풀 관리:
+
** ONTAP 스토리지를 Libvirt 스토리지 풀로 정의: NFS, iSCSI 또는 파이버 채널과 같은 프로토콜을 통해 ONTAP 볼륨이나 LUN을 가리키도록 Libvirt 스토리지 풀을 구성할 수 있습니다.
** Libvirt는 풀 내의 볼륨을 관리합니다. 스토리지 풀이 정의되면 Libvirt는 ONTAP LUN 또는 파일에 해당하는 풀 내의 볼륨 생성, 삭제, 복제 및 스냅샷을 관리할 수 있습니다.
+
*** 예: NFS 스토리지 풀: Libvirt 호스트가 ONTAP에서 NFS 공유를 마운트하는 경우 Libvirt에서 NFS 기반 스토리지 풀을 정의할 수 있으며, 그러면 공유에 있는 파일이 VM 디스크에 사용할 수 있는 볼륨으로 나열됩니다.




. 가상 머신 디스크 스토리지:
+
** ONTAP에 VM 디스크 이미지 저장: ONTAP 스토리지에 의해 지원되는 Libvirt 스토리지 풀 내에서 가상 머신 디스크 이미지(예: qcow2, raw)를 생성할 수 있습니다.
** ONTAP의 스토리지 기능 활용: VM 디스크가 ONTAP 볼륨에 저장되면 ONTAP의 데이터 보호(스냅샷, SnapMirror, SnapVault), 스토리지 효율성(중복 제거, 압축) 및 성능 기능의 이점을 자동으로 누릴 수 있습니다.


. 데이터 보호:
+
** 자동화된 데이터 보호: ONTAP은 스냅샷 및 SnapMirror와 같은 기능을 통해 자동화된 데이터 보호 기능을 제공합니다. 이를 통해 온프레미스, 원격 사이트 또는 클라우드에 있는 다른 ONTAP 스토리지에 귀중한 데이터를 복제하여 보호할 수 있습니다.
** RPO 및 RTO: ONTAP의 데이터 보호 기능을 사용하면 낮은 복구 지점 목표(RPO)와 빠른 복구 시간 목표(RTO)를 달성할 수 있습니다.
** MetroCluster/SnapMirror 액티브 동기화: 자동화된 0 RPO(복구 지점 목표) 및 사이트 간 가용성을 위해 ONTAP MetroCluster 또는 SMas를 사용할 수 있으며, 이를 통해 사이트 간에 스트레치 클러스터를 구축할 수 있습니다.


. 성능 및 효율성:
+
** Virtio 드라이버: 게스트 VM에서 Virtio 네트워크 및 디스크 장치 드라이버를 사용하여 성능을 향상시키세요. 이 드라이버는 하이퍼바이저와 연동하여 반가상화(Paravirtualization)의 이점을 제공하도록 설계되었습니다.
** Virtio-SCSI: 확장성과 고급 스토리지 기능을 위해 SCSI LUN에 직접 연결하고 많은 수의 장치를 처리할 수 있는 기능을 제공하는 Virtio-SCSI를 사용하세요.
** 스토리지 효율성: 중복 제거, 압축, 압축과 같은 ONTAP의 스토리지 효율성 기능은 VM 디스크의 스토리지 공간을 줄이는 데 도움이 되어 비용을 절감할 수 있습니다.


. ONTAP Select 통합:
+
** KVM에서의 ONTAP Select: NetApp의 소프트웨어 정의 스토리지 솔루션인 ONTAP Select는 KVM 호스트에 배포할 수 있어 Libvirt 기반 VM을 위한 유연하고 확장 가능한 스토리지 플랫폼을 제공합니다.
** ONTAP Select Deploy: ONTAP Select Deploy는 ONTAP Select 클러스터를 생성하고 관리하는 데 사용되는 도구입니다. KVM 또는 VMware ESXi에서 가상 머신으로 실행할 수 있습니다.




본질적으로 ONTAP과 함께 Libvirt를 사용하면 Libvirt 기반 가상화의 유연성과 확장성을 ONTAP의 엔터프라이즈급 데이터 관리 기능과 결합하여 가상화된 환경을 위한 강력하고 효율적인 솔루션을 제공할 수 있습니다.



== 파일 기반 스토리지 풀(SMB 또는 NFS 포함)

파일 기반 스토리지에는 dir 및 netfs 유형의 스토리지 풀을 적용할 수 있습니다.

[cols="20% 10% 10% 10% 10% 10% 10% 10%"]
|===
| 저장 프로토콜 | 디렉터 | 에프에스 | 넷에프에스 | 논리적 | 디스크 | iSCSI | iSCSI 직접 연결 | 엠패스 


| SMB/CIFS | 예 | 아니요 | 예 | 아니요 | 아니요 | 아니요 | 아니요 | 아니요 


| NFS 를 참조하십시오 | 예 | 아니요 | 예 | 아니요 | 아니요 | 아니요 | 아니요 | 아니요 
|===
netfs를 사용하면 libvirt가 파일 시스템을 마운트하며, 지원되는 마운트 옵션은 제한적입니다. dir 스토리지 풀을 사용하는 경우 파일 시스템 마운트는 호스트 외부에서 처리해야 합니다. fstab 또는 automounter를 사용하면 이러한 목적으로 사용할 수 있습니다. automounter를 사용하려면 autofs 패키지를 설치해야 합니다. autofs는 네트워크 공유를 필요에 따라 마운트하는 데 특히 유용하며, fstab의 정적 마운트보다 시스템 성능과 리소스 활용도를 향상시킬 수 있습니다. 일정 시간 동안 사용하지 않으면 자동으로 공유의 마운트를 해제합니다.

사용된 저장 프로토콜을 기반으로 호스트에 필요한 패키지가 설치되어 있는지 확인합니다.

[cols="40% 20% 20% 20%"]
|===
| 저장 프로토콜 | 페도라 | 데비안 | 팩맨 


| SMB/CIFS | 삼바 클라이언트/cifs-유틸리티 | smbclient/cifs-utils | smbclient/cifs-utils 


| NFS 를 참조하십시오 | nfs-유틸리티 | nfs-공통 | nfs-유틸리티 
|===
NFS는 Linux에서 기본 지원과 뛰어난 성능으로 인해 널리 사용되는 반면, SMB는 Microsoft 환경과 통합할 수 있는 실용적인 옵션입니다. 프로덕션 환경에서 사용하기 전에 항상 지원 목록을 확인하세요.

선택한 프로토콜에 따라 적절한 단계에 따라 SMB 공유 또는 NFS 내보내기를 만듭니다. https://docs.netapp.com/us-en/ontap-system-manager-classic/smb-config/index.html["SMB 주식 생성"]https://docs.netapp.com/us-en/ontap-system-manager-classic/nfs-config/index.html["NFS 내보내기 생성"]

fstab 또는 automounter 설정 파일에 마운트 옵션을 포함합니다. 예를 들어, autofs를 사용하는 경우, auto.kvmfs01 및 auto.kvmsmb01 파일을 사용하여 직접 매핑하기 위해 /etc/auto.master에 다음 줄을 포함했습니다.

/- /etc/auto.kvmnfs01 --타임아웃=60 /- /etc/auto.kvmsmb01 --타임아웃=60 --고스트

그리고 /etc/auto.kvmnfs01 파일에는 /mnt/kvmnfs01 -trunkdiscovery,nconnect=4 172.21.35.11,172.21.36.11(100):/kvmnfs01이 있었습니다.

smb의 경우 /etc/auto.kvmsmb01에 /mnt/kvmsmb01 -fstype=cifs,credentials=/root/smbpass,multichannel,max_channels=8 ://kvmfs01.sddc.netapp.com/kvmsmb01이 있습니다.

virsh를 사용하여 풀 유형 dir의 스토리지 풀을 정의합니다.

[source, shell]
----
virsh pool-define-as --name kvmnfs01 --type dir --target /mnt/kvmnfs01
virsh pool-autostart kvmnfs01
virsh pool-start kvmnfs01
----
기존 VM 디스크는 다음을 사용하여 나열할 수 있습니다.

[source, shell]
----
virsh vol-list kvmnfs01
----
NFS 마운트 기반 Libvirt 스토리지 풀의 성능을 최적화하기 위해서는 세션 트렁킹, pNFS, 그리고 nconnect 마운트 옵션의 세 가지 옵션이 모두 효과적일 수 있지만, 각각의 효과는 사용자의 특정 요구 사항과 환경에 따라 달라집니다. 최적의 접근 방식을 선택하는 데 도움이 되는 세부 사항은 다음과 같습니다.

. 연결 안 함:
+
** 가장 적합한 용도: 여러 TCP 연결을 사용하여 NFS 마운트 자체를 간단하고 직접적으로 최적화합니다.
** 작동 방식: nconnect 마운트 옵션을 사용하면 NFS 클라이언트가 NFS 엔드포인트(서버)와 설정할 TCP 연결 수를 지정할 수 있습니다. 이를 통해 여러 동시 연결을 통해 이점을 얻는 워크로드의 처리량을 크게 향상시킬 수 있습니다.
** 이익:
+
*** 구성하기 쉽습니다. NFS 마운트 옵션에 nconnect=<연결 수>를 추가하기만 하면 됩니다.
*** 처리량 향상: NFS 트래픽에 대한 "파이프 폭"이 늘어납니다.
*** 다양한 작업 부하에 효과적입니다. 일반적인 가상 머신 작업 부하에 유용합니다.


** 제한 사항:
+
*** 클라이언트/서버 지원: 클라이언트(Linux 커널)와 NFS 서버(예: ONTAP) 모두에서 nconnect에 대한 지원이 필요합니다.
*** 포화: nconnect 값을 너무 높게 설정하면 네트워크 회선이 포화될 수 있습니다.
*** 마운트별 설정: nconnect 값은 최초 마운트에 대해 설정되고, 동일한 서버와 버전에 대한 모든 후속 마운트는 이 값을 상속합니다.




. 세션 트렁킹:
+
** 가장 적합한 용도: NFS 서버에 여러 네트워크 인터페이스(LIF)를 활용하여 처리량을 높이고 일정 수준의 복원력을 제공합니다.
** 작동 방식: 세션 트렁킹을 사용하면 NFS 클라이언트가 NFS 서버의 여러 LIF에 여러 연결을 열어 여러 네트워크 경로의 대역폭을 효과적으로 집계할 수 있습니다.
** 이익:
+
*** 데이터 전송 속도 향상: 여러 네트워크 경로를 활용함으로써.
*** 복원력: 하나의 네트워크 경로에 장애가 발생하더라도 다른 경로는 계속 사용할 수 있습니다. 단, 장애가 발생한 경로에서 진행 중인 작업은 연결이 재설정될 때까지 중단될 수 있습니다.


** 제한 사항: 여전히 단일 NFS 세션입니다. 여러 네트워크 경로를 사용하지만 기존 NFS의 기본적인 단일 세션 특성은 변경되지 않습니다.
** 구성 복잡성: ONTAP 서버에서 트렁킹 그룹 및 LIF 구성이 필요합니다. 네트워크 설정: 다중 경로를 지원하는 적절한 네트워크 인프라가 필요합니다.
** nConnect 옵션 사용 시: 첫 번째 인터페이스에만 nConnect 옵션이 적용됩니다. 나머지 인터페이스는 단일 연결을 사용합니다.


. pNFS:
+
** 가장 적합한 대상: 병렬 데이터 액세스와 스토리지 장치에 대한 직접 I/O의 이점을 누릴 수 있는 고성능, 확장형 워크로드입니다.
** 작동 방식: pNFS는 메타데이터와 데이터 경로를 분리하여 클라이언트가 저장소에서 직접 데이터에 액세스할 수 있도록 하며, 잠재적으로 데이터 액세스를 위해 NFS 서버를 우회할 수 있습니다.
** 이익:
+
*** 향상된 확장성 및 성능: HPC 및 AI/ML과 같이 병렬 I/O의 이점을 얻는 특정 워크로드에 적합합니다.
*** 직접 데이터 액세스: 클라이언트가 저장소에서 직접 데이터를 읽고 쓸 수 있도록 하여 대기 시간을 줄이고 성능을 향상시킵니다.
*** nConnect 옵션 사용: 모든 연결에 nConnect가 적용되어 네트워크 대역폭을 극대화합니다.


** 제한 사항:
+
*** 복잡성: pNFS는 기존 NFS나 nconnect보다 설정 및 관리가 더 복잡합니다.
*** 작업 부하별: 모든 작업 부하가 pNFS로부터 상당한 이점을 얻는 것은 아닙니다.
*** 클라이언트 지원: 클라이언트 측에서 pNFS 지원이 필요합니다.






권장 사항: * NFS 기반 범용 Libvirt 스토리지 풀의 경우: nconnect 마운트 옵션으로 시작하세요. 구현이 비교적 쉽고 연결 수를 늘려 성능을 크게 향상시킬 수 있습니다. * 더 높은 처리량과 복원력이 필요한 경우: nconnect와 함께 또는 nconnect 대신 세션 트렁킹을 사용하는 것을 고려해 보세요. Libvirt 호스트와 ONTAP 시스템 간에 여러 네트워크 인터페이스가 있는 환경에서 세션 트렁킹은 유용할 수 있습니다. * 병렬 I/O의 이점을 활용하는 까다로운 워크로드의 경우: HPC 또는 AI/ML과 같이 병렬 데이터 액세스를 활용할 수 있는 워크로드를 실행하는 경우 pNFS가 가장 적합한 옵션일 수 있습니다. 하지만 설정 및 구성이 복잡해질 수 있다는 점에 유의해야 합니다. 특정 Libvirt 스토리지 풀 및 워크로드에 가장 적합한 구성을 결정하려면 다양한 마운트 옵션과 설정을 사용하여 NFS 성능을 테스트하고 모니터링하세요.



== 블록 기반 스토리지 풀(iSCSI, FC 또는 NVMe-oF 포함)

디렉토리 풀 유형은 종종 공유 LUN이나 네임스페이스의 OCFS2나 GFS2와 같은 클러스터 파일 시스템의 상위에서 사용됩니다.

사용된 저장 프로토콜에 따라 호스트에 필요한 패키지가 설치되어 있는지 확인합니다.

[cols="40% 20% 20% 20%"]
|===
| 저장 프로토콜 | 페도라 | 데비안 | 팩맨 


| iSCSI | iSCSI 이니시에이터 유틸리티, 디바이스 매퍼 멀티패스, OCF2 도구/GFS2 유틸리티 | open-iscsi, multipath-tools, ocfs2-tools/gfs2-utils | open-iscsi, multipath-tools, ocfs2-tools/gfs2-utils 


| FC | 장치 매퍼 다중 경로, ocfs2 도구/gfs2 유틸리티 | 멀티패스 도구, ocfs2 도구/gfs2 유틸리티 | 멀티패스 도구, ocfs2 도구/gfs2 유틸리티 


| NVMe - oF | nvme-cli, ocfs2-tools/gfs2-utils | nvme-cli, ocfs2-tools/gfs2-utils | nvme-cli, ocfs2-tools/gfs2-utils 
|===
호스트 iqn/wwpn/nqn을 수집합니다.

[source, shell]
----
# To view host iqn
cat /etc/iscsi/initiatorname.iscsi
# To view wwpn
systool -c fc_host -v
# or if you have ONTAP Linux Host Utility installed
sanlun fcp show adapter -v
# To view nqn
sudo nvme show-hostnqn
----
LUN이나 네임스페이스를 생성하려면 해당 섹션을 참조하세요.

https://docs.netapp.com/us-en/ontap-system-manager-classic/iscsi-config-rhel/index.html["iSCSI 호스트에 대한 LUN 생성"] https://docs.netapp.com/us-en/ontap-system-manager-classic/fc-config-rhel/index.html["FC 호스트에 대한 LUN 생성"] https://docs.netapp.com/us-en/ontap/san-admin/create-nvme-namespace-subsystem-task.html["NVMe-oF 호스트를 위한 네임스페이스 생성"]

FC 구역화 또는 이더넷 장치가 ONTAP 논리 인터페이스와 통신하도록 구성되어 있는지 확인하세요.

iSCSI의 경우,

[source, shell]
----
# Register the target portal
iscsiadm -m discovery -t st -p 172.21.37.14
# Login to all interfaces
iscsiadm -m node -L all
# Ensure iSCSI service is enabled
sudo systemctl enable iscsi.service
# Verify the multipath device info
multipath -ll
# OCFS2 configuration we used.
o2cb add-cluster kvmcl01
o2cb add-node kvm02.sddc.netapp.com
o2cb cluster-status
mkfs.ocfs2 -L vmdata -N 4  --cluster-name=kvmcl01 --cluster-stack=o2cb -F /dev/mapper/3600a098038314c57312b58387638574f
mount -t ocfs2 /dev/mapper/3600a098038314c57312b58387638574f1 /mnt/kvmiscsi01/
mounted.ocfs2 -d
# For libvirt storage pool
virsh pool-define-as --name kvmiscsi01 --type dir --target /mnt/kvmiscsi01
virsh pool-autostart kvmiscsi01
virsh pool-start kvmiscsi01
----
NVMe/TCP의 경우 다음을 사용했습니다.

[source, shell]
----
# Listing the NVMe discovery
cat /etc/nvme/discovery.conf
# Used for extracting default parameters for discovery
#
# Example:
# --transport=<trtype> --traddr=<traddr> --trsvcid=<trsvcid> --host-traddr=<host-traddr> --host-iface=<host-iface>
-t tcp -l 1800 -a 172.21.37.16
-t tcp -l 1800 -a 172.21.37.17
-t tcp -l 1800 -a 172.21.38.19
-t tcp -l 1800 -a 172.21.38.20
# Login to all interfaces
nvme connect-all
nvme list
# Verify the multipath device info
nvme show-topology
# OCFS2 configuration we used.
o2cb add-cluster kvmcl01
o2cb add-node kvm02.sddc.netapp.com
o2cb cluster-status
mkfs.ocfs2 -L vmdata1 -N 4  --cluster-name=kvmcl01 --cluster-stack=o2cb -F /dev/nvme2n1
mount -t ocfs2 /dev/nvme2n1 /mnt/kvmns01/
mounted.ocfs2 -d
# To change label
tunefs.ocfs2 -L tme /dev/nvme2n1
# For libvirt storage pool
virsh pool-define-as --name kvmns01 --type dir --target /mnt/kvmns01
virsh pool-autostart kvmns01
virsh pool-start kvmns01
----
FC의 경우

[source, shell]
----
# Verify the multipath device info
multipath -ll
# OCFS2 configuration we used.
o2cb add-cluster kvmcl01
o2cb add-node kvm02.sddc.netapp.com
o2cb cluster-status
mkfs.ocfs2 -L vmdata2 -N 4  --cluster-name=kvmcl01 --cluster-stack=o2cb -F /dev/mapper/3600a098038314c57312b583876385751
mount -t ocfs2 /dev/mapper/3600a098038314c57312b583876385751 /mnt/kvmfc01/
mounted.ocfs2 -d
# For libvirt storage pool
virsh pool-define-as --name kvmfc01 --type dir --target /mnt/kvmfc01
virsh pool-autostart kvmfc01
virsh pool-start kvmfc01
----
참고: 장치 마운트는 /etc/fstab에 포함되어야 하거나 자동 마운트 맵 파일을 사용해야 합니다.

Libvirt는 클러스터형 파일 시스템(OCFS2 또는 GFS2) 기반의 가상 디스크(파일)를 관리합니다. 기본 공유 블록 액세스 및 데이터 무결성을 처리하기 위해 클러스터형 파일 시스템(OCFS2 또는 GFS2)을 사용합니다. OCFS2 또는 GFS2는 Libvirt 호스트와 공유 블록 스토리지 간의 추상화 계층 역할을 하며, 해당 공유 스토리지에 저장된 가상 디스크 이미지에 대한 안전한 동시 액세스를 허용하는 데 필요한 잠금 및 조정 기능을 제공합니다.
