---
sidebar: sidebar 
permalink: data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html 
keywords: AWS cloud, ha pair, high availability, openmessage benchmarking, architectural setup 
summary: NetApp NFS에 스토리지 계층이 마운트된 Kafka 클러스터가 AWS 클라우드의 성능을 벤치마킹했습니다. 벤치마킹 예는 다음 섹션에 설명되어 있습니다. 
---
= AWS의 성능 개요 및 검증
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:kafka-nfs-why-netapp-nfs-for-kafka-workloads.html["이전: Kafka 워크로드를 위한 NetApp NFS를 선택해야 하는 이유"]

[role="lead"]
NetApp NFS에 스토리지 계층이 마운트된 Kafka 클러스터가 AWS 클라우드의 성능을 벤치마킹했습니다. 벤치마킹 예는 다음 섹션에 설명되어 있습니다.



== AWS 클라우드의 Kafka 및 NetApp Cloud Volumes ONTAP(고가용성 쌍 및 단일 노드)

NetApp Cloud Volumes ONTAP(HA 쌍)를 사용한 Kafka 클러스터는 AWS 클라우드의 성능을 벤치마킹했습니다. 이 벤치마킹은 다음 섹션에서 설명합니다.



=== 아키텍처 설정

다음 표에는 NAS를 사용하는 Kafka 클러스터의 환경 구성이 나와 있습니다.

|===
| 플랫폼 구성 요소 | 환경 구성 


| Kafka 3.2.3  a| 
* 3 x zookeers – T2.small
* 브로커 서버 3대 – i3en.2xLarge
* Grafana 1개 – c5n.2xLarge
* 생산자/소비자 4대 -- c5n.2xLarge *




| 모든 노드의 운영 체제 | RHEL8.6 


| NetApp Cloud Volumes ONTAP 인스턴스 | HA 쌍 인스턴스 – m5dn.12xLargge x 2노드 단일 노드 인스턴스 - m5dn.12xLargge x 1개 노드 
|===


=== NetApp 클러스터 볼륨 ONTAP 설정

. Cloud Volumes ONTAP HA 쌍의 경우, 각 스토리지 컨트롤러에서 각 애그리게이트의 볼륨 3개가 포함된 애그리게이트 2개를 생성했습니다. 단일 Cloud Volumes ONTAP 노드의 경우 Aggregate에 6개의 볼륨을 생성합니다.
+
image:kafka-nfs-image25.png["이 이미지는 aggr3 및 aggr22의 속성을 보여줍니다."]

+
image:kafka-nfs-image26.png["이 이미지는 aggr2의 속성을 보여줍니다."]

. 네트워크 성능을 향상시키기 위해 HA 쌍과 단일 노드 모두에 고속 네트워킹을 활성화했습니다.
+
image:kafka-nfs-image27.png["이 이미지는 고속 네트워킹을 활성화하는 방법을 보여줍니다."]

. ONTAP NVRAM의 IOPS가 더 많으므로 Cloud Volumes ONTAP 루트 볼륨의 IOPS를 2350으로 변경했습니다. Cloud Volumes ONTAP의 루트 볼륨 디스크 크기는 47GB였습니다. 다음 ONTAP 명령은 HA 쌍용이며 단일 노드에도 동일한 단계를 적용할 수 있습니다.
+
....
statistics start -object vnvram -instance vnvram -counter backing_store_iops -sample-id sample_555
kafka_nfs_cvo_ha1::*> statistics show -sample-id sample_555
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-01
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1479
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-02
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1210
2 entries were displayed.
kafka_nfs_cvo_ha1::*>
....
+
image:kafka-nfs-image28.png["이 이미지는 볼륨 속성을 수정하는 방법을 보여줍니다."]



다음 그림에서는 NAS 기반 Kafka 클러스터의 아키텍처를 보여 줍니다.

* * Compute. * 3노드 Kafka 클러스터와 전용 서버에서 실행되는 3노드 zookeeper 앙상블이 사용되었습니다. 각 브로커에는 전용 LIF를 통해 Cloud Volumes ONTAP 인스턴스의 단일 볼륨에 대한 NFS 마운트 지점이 2개 있었습니다.
* * 모니터링. * Prometheus-Grafana 조합에 두 개의 노드를 사용했습니다. 워크로드를 생성하는데 이 Kafka 클러스터를 생성하고 사용할 수 있는 별도의 3노드 클러스터를 사용했습니다.
* * 스토리지. * 인스턴스에 하나의 6TB GP3 AWS-EBS 볼륨이 마운트된 HA 쌍 Cloud Volumes ONTAP 인스턴스를 사용했습니다. 그런 다음 NFS 마운트를 사용하여 Kafka 브로커로 볼륨을 내보냅니다.


image:kafka-nfs-image29.png["이 그림은 NAS 기반 Kafka 클러스터의 아키텍처를 보여 줍니다."]



=== Openmessage 벤치마킹 구성

. NFS 성능을 개선하려면 nconnect를 사용하여 생성할 수 있는 NFS 서버와 NFS 클라이언트 간에 더 많은 네트워크 연결이 필요합니다. 다음 명령을 실행하여 브로커 노드에서 nconnect 옵션으로 NFS 볼륨을 마운트합니다.
+
....
[root@ip-172-30-0-121 ~]# cat /etc/fstab
UUID=eaa1f38e-de0f-4ed5-a5b5-2fa9db43bb38/xfsdefaults00
/dev/nvme1n1 /mnt/data-1 xfs defaults,noatime,nodiscard 0 0
/dev/nvme2n1 /mnt/data-2 xfs defaults,noatime,nodiscard 0 0
172.30.0.233:/kafka_aggr3_vol1 /kafka_aggr3_vol1 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol2 /kafka_aggr3_vol2 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol3 /kafka_aggr3_vol3 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol1 /kafka_aggr22_vol1 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol2 /kafka_aggr22_vol2 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol3 /kafka_aggr22_vol3 nfs defaults,nconnect=16 0 0
[root@ip-172-30-0-121 ~]# mount -a
[root@ip-172-30-0-121 ~]# df -h
Filesystem                       Size  Used Avail Use% Mounted on
devtmpfs                          31G     0   31G   0% /dev
tmpfs                             31G  249M   31G   1% /run
tmpfs                             31G     0   31G   0% /sys/fs/cgroup
/dev/nvme0n1p2                    10G  2.8G  7.2G  28% /
/dev/nvme1n1                     2.3T  248G  2.1T  11% /mnt/data-1
/dev/nvme2n1                     2.3T  245G  2.1T  11% /mnt/data-2
172.30.0.233:/kafka_aggr3_vol1   1.0T   12G 1013G   2% /kafka_aggr3_vol1
172.30.0.233:/kafka_aggr3_vol2   1.0T  5.5G 1019G   1% /kafka_aggr3_vol2
172.30.0.233:/kafka_aggr3_vol3   1.0T  8.9G 1016G   1% /kafka_aggr3_vol3
172.30.0.242:/kafka_aggr22_vol1  1.0T  7.3G 1017G   1% /kafka_aggr22_vol1
172.30.0.242:/kafka_aggr22_vol2  1.0T  6.9G 1018G   1% /kafka_aggr22_vol2
172.30.0.242:/kafka_aggr22_vol3  1.0T  5.9G 1019G   1% /kafka_aggr22_vol3
tmpfs                            6.2G     0  6.2G   0% /run/user/1000
[root@ip-172-30-0-121 ~]#
....
. Cloud Volumes ONTAP에서 네트워크 연결을 확인합니다. 다음 ONTAP 명령은 단일 Cloud Volumes ONTAP 노드에서 사용됩니다. Cloud Volumes ONTAP HA 쌍에도 동일한 단계가 적용됩니다.
+
....
Last login time: 1/20/2023 00:16:29
kafka_nfs_cvo_sn::> network connections active show -service nfs* -fields remote-host
node                cid        vserver              remote-host
------------------- ---------- -------------------- ------------
kafka_nfs_cvo_sn-01 2315762628 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762629 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762630 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762631 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762632 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762633 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762634 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762635 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762636 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762637 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762639 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762640 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762641 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762642 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762643 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762644 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762645 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762646 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762647 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762648 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762649 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762650 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762651 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762652 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762653 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762656 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762657 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762658 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762659 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762660 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762661 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762662 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762663 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762664 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762665 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762666 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762667 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762668 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762669 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762670 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762671 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762672 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762673 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762674 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762676 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762677 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762678 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762679 svm_kafka_nfs_cvo_sn 172.30.0.223
48 entries were displayed.
 
kafka_nfs_cvo_sn::>
....
. 다음 Kafka를 사용합니다 `server.properties` 모든 Kafka 브로커는 Cloud Volumes ONTAP HA 페어를 지원합니다. 를 클릭합니다 `log.dirs` 각 브로커에 따라 속성이 다르며 나머지 속성은 브로커에 공통입니다. broker1의 경우, 를 참조하십시오 `log.dirs` 값은 다음과 같습니다.
+
....
[root@ip-172-30-0-121 ~]# cat /opt/kafka/config/server.properties
broker.id=0
advertised.listeners=PLAINTEXT://172.30.0.121:9092
#log.dirs=/mnt/data-1/d1,/mnt/data-1/d2,/mnt/data-1/d3,/mnt/data-2/d1,/mnt/data-2/d2,/mnt/data-2/d3
log.dirs=/kafka_aggr3_vol1/broker1,/kafka_aggr3_vol2/broker1,/kafka_aggr3_vol3/broker1,/kafka_aggr22_vol1/broker1,/kafka_aggr22_vol2/broker1,/kafka_aggr22_vol3/broker1
zookeeper.connect=172.30.0.12:2181,172.30.0.30:2181,172.30.0.178:2181
num.network.threads=64
num.io.threads=64
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
replica.fetch.max.bytes=524288000
background.threads=20
num.replica.alter.log.dirs.threads=40
num.replica.fetchers=20
[root@ip-172-30-0-121 ~]#
....
+
** broker2의 경우, 를 참조하십시오 `log.dirs` 속성 값은 다음과 같습니다.
+
....
log.dirs=/kafka_aggr3_vol1/broker2,/kafka_aggr3_vol2/broker2,/kafka_aggr3_vol3/broker2,/kafka_aggr22_vol1/broker2,/kafka_aggr22_vol2/broker2,/kafka_aggr22_vol3/broker2
....
** broker3의 경우, 를 참조하십시오 `log.dirs` 속성 값은 다음과 같습니다.
+
....
log.dirs=/kafka_aggr3_vol1/broker3,/kafka_aggr3_vol2/broker3,/kafka_aggr3_vol3/broker3,/kafka_aggr22_vol1/broker3,/kafka_aggr22_vol2/broker3,/kafka_aggr22_vol3/broker3
....


. 단일 Cloud Volumes ONTAP 노드의 경우 Kafka입니다 `servers.properties` 은(는) 를 제외하고 Cloud Volumes ONTAP HA 쌍과 동일합니다 `log.dirs` 속성.
+
** broker1의 경우, 를 참조하십시오 `log.dirs` 값은 다음과 같습니다.
+
....
log.dirs=/kafka_aggr2_vol1/broker1,/kafka_aggr2_vol2/broker1,/kafka_aggr2_vol3/broker1,/kafka_aggr2_vol4/broker1,/kafka_aggr2_vol5/broker1,/kafka_aggr2_vol6/broker1
....
** broker2의 경우, 를 참조하십시오 `log.dirs` 값은 다음과 같습니다.
+
....
log.dirs=/kafka_aggr2_vol1/broker2,/kafka_aggr2_vol2/broker2,/kafka_aggr2_vol3/broker2,/kafka_aggr2_vol4/broker2,/kafka_aggr2_vol5/broker2,/kafka_aggr2_vol6/broker2
....
** broker3의 경우, 를 참조하십시오 `log.dirs` 속성 값은 다음과 같습니다.
+
....
log.dirs=/kafka_aggr2_vol1/broker3,/kafka_aggr2_vol2/broker3,/kafka_aggr2_vol3/broker3,/kafka_aggr2_vol4/broker3,/kafka_aggr2_vol5/broker3,/kafka_aggr2_vol6/broker3
....


. OMB의 워크로드는 다음과 같은 속성으로 구성됩니다. `(/opt/benchmark/workloads/1-topic-100-partitions-1kb.yaml)`.
+
....
topics: 4
partitionsPerTopic: 100
messageSize: 32768
useRandomizedPayloads: true
randomBytesRatio: 0.5
randomizedPayloadPoolSize: 100
subscriptionsPerTopic: 1
consumerPerSubscription: 80
producersPerTopic: 40
producerRate: 1000000
consumerBacklogSizeGB: 0
testDurationMinutes: 5
....
+
를 클릭합니다 `messageSize` 사용 사례마다 다를 수 있습니다. 성능 테스트에서는 3K를 사용했습니다.

+
OMB에서 서로 다른 두 드라이버, 즉 Sync 또는 Throughput을 사용하여 Kafka 클러스터에서 워크로드를 생성했습니다.

+
** Sync 드라이버 속성에 사용되는 YAML 파일은 다음과 같습니다 `(/opt/benchmark/driver- kafka/kafka-sync.yaml)`:
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
  flush.messages=1
  flush.ms=0
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....
** 처리량 운전자 속성에 사용되는 YAML 파일은 다음과 같습니다 `(/opt/benchmark/driver- kafka/kafka-throughput.yaml)`:
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
  default.api.timeout.ms=1200000
  request.timeout.ms=1200000
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....






== 테스트 방법

. Kafka 클러스터는 Terraform 및 Ansible을 사용하여 위에서 설명한 사양에 따라 프로비저닝되었습니다. Terraform은 Kafka 클러스터용 AWS 인스턴스를 사용하여 인프라를 구축하는 데 사용되며, Ansible은 Kafka 클러스터를 기반으로 합니다.
. 위에 설명된 워크로드 구성과 동기화 드라이버로 OMB 워크로드가 트리거되었습니다.
+
....
Sudo bin/benchmark –drivers driver-kafka/kafka- sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. 동일한 워크로드 구성의 처리량 드라이버에서 또 다른 워크로드가 트리거되었습니다.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




== 관찰

NFS에서 실행되는 Kafka 인스턴스의 성능을 벤치마크하는 워크로드를 생성하는 데 두 가지 유형의 드라이버가 사용되었습니다. 드라이버의 차이점은 로그 플러시 속성입니다.

Cloud Volumes ONTAP HA 쌍:

* Sync 드라이버에서 일관되게 생성된 총 처리량: ~1236Mbps.
* 처리량 드라이버에 대해 생성된 총 처리량: 최대 1412Mbps.


단일 Cloud Volumes ONTAP 노드의 경우:

* Sync 드라이버에서 일관되게 생성된 총 처리량: ~1962MBps
* 처리량 드라이버에서 생성된 총 처리량: 최대 1660MBps


Sync 드라이버는 로그가 디스크에 즉시 플러시될 때 일관된 처리량을 생성할 수 있는 반면, 처리량 드라이버는 로그가 대량으로 디스크에 커밋될 때 처리량 버스트를 생성합니다.

이러한 처리량 수치는 지정된 AWS 구성에 대해 생성됩니다. 더 높은 성능 요구 사항을 위해 더 나은 처리량 수치를 위해 인스턴스 유형을 확장하고 조정할 수 있습니다. 총 처리량 또는 총 속도는 생산자와 소비자 속도의 조합입니다.

image:kafka-nfs-image30.png["여기에 네 개의 서로 다른 그래프가 표시됩니다. CVO-HA Pair 처리량 드라이버 CVO-HA 쌍 동기화 드라이버. CVO - 단일 노드 처리량 드라이버 CVO - 단일 노드 동기화 드라이버"]

처리량 또는 동기화 드라이버 벤치마킹 수행 시 스토리지 처리량을 확인하십시오.

image:kafka-nfs-image31.png["이 그래프는 지연 시간, IOPS 및 처리량의 성능을 보여 줍니다."]



== AWS FSxN의 Apache Kafka



=== 개요

NFS(Network File System)는 대량의 데이터를 저장하는 데 널리 사용되는 네트워크 파일 시스템입니다. 대부분의 조직에서 데이터는 Apache Kafka와 같은 스트리밍 애플리케이션에 의해 점점 더 많이 생성되고 있습니다. 이러한 워크로드는 최신 스토리지 기능을 갖춘 확장성, 짧은 지연 시간, 강력한 데이터 수집 아키텍처를 필요로 합니다. 실시간 분석을 지원하고 실행 가능한 통찰력을 제공하기 위해 우수한 설계 및 고성능 인프라가 필요합니다.

Kafka는 POSIX 호환 파일 시스템과 호환되며 파일 시스템에 의존하여 파일 작업을 처리하지만 NFSv3 파일 시스템에 데이터를 저장할 때 Kafka 브로커 NFS 클라이언트는 XFS 또는 ext4 같은 로컬 파일 시스템과 다르게 파일 작업을 해석할 수 있습니다. 일반적인 예로는 클러스터를 확장하고 파티션을 다시 할당할 때 Kafka 브로커가 실패하는 NFS의 이름이 있습니다. 이러한 문제를 해결하기 위해 NetApp은 RHEL8.7, RHEL9.1에서 일반적으로 사용할 수 있는 변경 사항으로 오픈 소스 Linux NFS 클라이언트를 업데이트했으며 현재 ONTAP용 FSx 릴리즈 ONTAP 9.12.1에서 지원합니다.

NetApp ONTAP용 Amazon FSx는 클라우드에서 완벽하게 관리되고 확장 가능하며 뛰어난 성능의 NFS 파일 시스템을 제공합니다. NetApp용 FSx의 Kafka 데이터는 대량의 데이터를 처리하고 내결함성을 보장하기 위해 확장할 수 있습니다. NFS는 중요하거나 민감한 데이터 세트에 대해 중앙 집중식 스토리지 관리 및 데이터 보호를 제공합니다.

이러한 향상된 기능을 통해 AWS 고객은 AWS 컴퓨팅 서비스에서 Kafka 워크로드를 실행할 때 ONTAP용 FSx를 활용할 수 있습니다. 다음과 같은 이점을 제공합니다.
* CPU 활용률을 줄여 I/O 대기 시간을 줄입니다
* 더 빠른 Kafka 브로커 복구 시간
* 안정성 및 효율성
* 확장성 및 성능
* 다중 가용성 영역 가용성
* 데이터 보호



=== AWS FSxN에서의 성능 개요 및 검증

NetApp NFS에 스토리지 계층이 마운트된 Kafka 클러스터가 AWS FSxN의 성능을 벤치마킹했습니다. 벤치마킹 예는 다음 섹션에 설명되어 있습니다.



==== AWS FSxN의 Kafka(액티브 패시브)

AWS FSxN이 포함된 Kafka 클러스터는 AWS 클라우드의 성능을 벤치마킹했습니다. 이 벤치마킹은 다음 섹션에서 설명합니다.



==== 아키텍처 설정

다음 표에는 AWS FSxN을 사용하는 Kafka 클러스터의 환경 구성이 나와 있습니다.

|===
| 플랫폼 구성 요소 | 환경 구성 


| Kafka 3.2.3  a| 
* 3 x zookeers – T2.small
* 브로커 서버 3대 – i3en.2xLarge
* Grafana 1개 – c5n.2xLarge
* 생산자/소비자 4대 -- c5n.2xLarge *




| 모든 노드의 운영 체제 | RHEL8.6 


| AWS FSxN | 4GB/s 처리량과 160000 IPS를 지원하는 액티브 패시브 인스턴스 
|===


==== NetApp FSxN 설정

. 초기 테스트를 위해 2TB 및 40000 IOPS의 NetApp ONTAP 파일 시스템용 FSx를 2GB/sec 처리량으로 만들었습니다.
. NetApp ONTAP용 FSx에서 테스트 영역(US-East-1)의 2GB/sec 처리량 파일 시스템에 대해 달성 가능한 최대 IOPS는 80,000 IOPS입니다. NetApp ONTAP 파일 시스템용 FSx의 총 최대 IOPS는 160,000 IOPS이며, 이를 위해서는 4GB/sec의 처리량 구축이 필요하며, 이 문서의 후반부에 이 내용이 설명되어 있습니다
+
....
[root@ip-172-31-33-69 ~]# aws fsx create-file-system --region us-east-2  --storage-capacity 2048 --subnet-ids <desired subnet 1> subnet-<desired subnet 2> --file-system-type ONTAP --ontap-configuration DeploymentType=MULTI_AZ_HA_1,ThroughputCapacity=2048,PreferredSubnetId=<desired primary subnet>,FsxAdminPassword=<new password>,DiskIopsConfiguration="{Mode=USER_PROVISIONED,Iops=40000"}
....
+
FSx "create-file-system"에 대한 자세한 명령줄 구문은 여기에서 찾을 수 있습니다. https://docs.aws.amazon.com/cli/latest/reference/fsx/create-file-system.html[]
예를 들어 KMS 키를 지정하지 않을 때 사용되는 기본 FSx 마스터 키가 아니라 특정 KMS 키를 지정할 수 있습니다.

. 다음과 같이 파일 시스템을 설명한 후 JSON의 "LifeCycle" 상태가 "Available"으로 변경될 때까지 기다립니다.
+
....
[root@ip-172-31-33-69 ~]# aws fsx describe-file-systems  --region us-east-1 --file-system-ids fs-02ff04bab5ce01c7c
....
. fsxadmin의 암호는 파일 시스템을 처음 생성할 때 구성된 암호입니다.
. fsxadmin을 통해 FsxN에 로그인하여 자격 증명을 검증합니다
+
....
[root@ip-172-31-33-69 ~]# ssh fsxadmin@198.19.250.244
The authenticity of host '198.19.250.244 (198.19.250.244)' can't be established.
ED25519 key fingerprint is SHA256:mgCyRXJfWRc2d/jOjFbMBsUcYOWjxoIky0ltHvVDL/Y.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '198.19.250.244' (ED25519) to the list of known hosts.
(fsxadmin@198.19.250.244) Password:

This is your first recorded login.
....
. FSxN에 스토리지 가상 머신을 생성합니다
+
....
[root@ip-172-31-33-69 ~]# aws fsx --region us-east-1 create-storage-virtual-machine --name svmkafkatest --file-system-id fs-02ff04bab5ce01c7c
....
. 새로 생성된 NetApp ONTAP 파일 시스템용 FSx로 SSH를 수행하고 아래 샘플 명령을 사용하여 스토리지 가상 머신에 볼륨을 생성합니다. 마찬가지로 이 검증을 위해 6개의 볼륨을 생성합니다. 당사의 검증을 기반으로 Kafka에 더 나은 성능을 제공하는 기본 구성 요소(8) 이하를 유지합니다.
+
....
FsxId02ff04bab5ce01c7c::*> volume create -volume kafkafsxN1 -state online -policy default -unix-permissions ---rwxr-xr-x -junction-active true -type RW -snapshot-policy none  -junction-path /kafkafsxN1 -aggr-list aggr1
....
. 볼륨의 크기를 2TB로 확장하고 접합 경로에 마운트합니다.
+
....
FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN1 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN1" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN2 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN2" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN3 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN3" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN4 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN4" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN5 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN5" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN6 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN6" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume show -vserver svmkafkatest -volume *
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
svmkafkatest
          kafkafsxN1   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN2   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN3   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN4   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN5   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN6   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          svmkafkatest_root
                       aggr1        online     RW          1GB    968.1MB    0%
7 entries were displayed.

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN1 -junction-path /kafkafsxN1

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN2 -junction-path /kafkafsxN2

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN3 -junction-path /kafkafsxN3

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN4 -junction-path /kafkafsxN4

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN5 -junction-path /kafkafsxN5

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN6 -junction-path /kafkafsxN6
....
. FSxN 처리량 용량을 2GB/sec에서 4GB/sec로, IOPS는 160000으로 확장합니다
+
....
[root@ip-172-31-33-69 ~]# aws fsx update-file-system --region us-east-1  --storage-capacity 5120 --ontap-configuration 'ThroughputCapacity=4096,DiskIopsConfiguration={Mode=USER_PROVISIONED,Iops=160000}' --file-system-id fs-02ff04bab5ce01c7c
....
+
FSx "update-file-system"에 대한 자세한 명령줄 구문은 여기에서 찾을 수 있습니다.
https://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html[]

. FSxN 볼륨은 kafkar 브로커의 nconnect 및 기본 opions로 마운트됩니다
+
image:aws-fsx-kafka-arch1.png["이 이미지는 FSxN 기반 Kafka 클러스터의 아키텍처를 보여줍니다."]

+
** 컴퓨팅. 3노드 Kafka 클러스터를 전용 서버에서 실행되는 3노드 zookeeper 앙상블과 함께 사용했습니다. 각 브로커는 FSxN 인스턴스의 6개 볼륨에 6개의 NFS 마운트 지점을 가지고 있었습니다.
** 모니터링. 두 개의 노드를 사용하여 Prometheus-Grafana 조합을 사용했습니다. 워크로드를 생성하는데 이 Kafka 클러스터를 생성하고 사용할 수 있는 별도의 3노드 클러스터를 사용했습니다.
** 스토리지. 1TB 볼륨 6개가 마운트된 FSxN을 사용했습니다. 그런 다음 NFS 마운트를 사용하여 Kafka 브로커로 볼륨을 내보냅니다.






==== Openmessage 벤치마킹 구성.

NetApp 클라우드 볼륨 ONTAP에 사용된 것과 동일한 구성을 사용했고 세부 정보는 여기 를 참조하십시오.
https://docs.netapp.com/us-en/netapp-solutions/data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html#architectural-setup[]



==== 테스트 방법

. Kafka 클러스터는 Terraform 및 Ansible을 사용하여 위에서 설명한 사양에 따라 프로비저닝되었습니다. Terraform은 Kafka 클러스터용 AWS 인스턴스를 사용하여 인프라를 구축하는 데 사용되며, Ansible은 Kafka 클러스터를 기반으로 합니다.
. 위에 설명된 워크로드 구성과 동기화 드라이버로 OMB 워크로드가 트리거되었습니다.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. 동일한 워크로드 구성의 처리량 드라이버에서 또 다른 워크로드가 트리거되었습니다.
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




==== 관찰

NFS에서 실행되는 Kafka 인스턴스의 성능을 벤치마크하는 워크로드를 생성하는 데 두 가지 유형의 드라이버가 사용되었습니다. 드라이버의 차이점은 로그 플러시 속성입니다.

Kafka 복제 계수 1 및 FSxN의 경우:

* Sync 드라이버에서 일관되게 생성된 총 처리량: ~3218Mbps 및 최대 성능: ~3652Mbps.
* 처리량 드라이버에서 일관되게 생성된 총 처리량: ~3679Mbps 및 최대 성능: ~3908Mbps.


복제 계수 3 및 FSxN을 사용하는 Kafka의 경우:

* Sync 드라이버에서 일관되게 생성된 총 처리량: ~1252Mbps 및 최대 성능: ~1382Mbps.
* 처리량 드라이버에서 일관되게 생성된 총 처리량: ~1218Mbps 및 최대 성능: ~1328Mbps.


Kafka 복제 계수 3에서 읽기 및 쓰기 작업이 FSxN에서 세 번 발생했으며 Kafka 복제 계수 1에서 읽기 및 쓰기 작업은 FSxN에서 한 번 발생하므로 두 검증 모두에서 최대 4GB/sec의 처리량을 달성할 수 있습니다.

Sync 드라이버는 로그가 디스크에 즉시 플러시될 때 일관된 처리량을 생성할 수 있는 반면, 처리량 드라이버는 로그가 대량으로 디스크에 커밋될 때 처리량 버스트를 생성합니다.

이러한 처리량 수치는 지정된 AWS 구성에 대해 생성됩니다. 더 높은 성능 요구 사항을 위해 더 나은 처리량 수치를 위해 인스턴스 유형을 확장하고 조정할 수 있습니다. 총 처리량 또는 총 속도는 생산자와 소비자 속도의 조합입니다.

image:aws-fsxn-performance-rf-1-rf-3.png["이 이미지는 RF1 및 RF3을 사용한 Kafka의 성능을 보여줍니다"]

아래 차트는 Kafka 복제 계수 3에 대한 2GB/sec FSxn 및 4GB/sec 성능을 보여줍니다. 복제 계수 3은 FSxN 스토리지에서 읽기 및 쓰기 작업을 세 번 수행합니다. 처리량 드라이버의 총 속도는 881MB/sec이며, Kafka는 2GB/sec FSxN 파일 시스템에서 약 2.64GB/sec의 읽기 및 쓰기 작업을 수행하고 처리량 드라이버의 총 속도는 1328MB/sec이며, Kafka는 약 3.98GB/sec의 읽기 및 쓰기 작업을 수행합니다. 또한 Kafka 성능은 FSxN 처리량을 기반으로 선형적으로 확장 가능합니다.

image:aws-fsxn-2gb-4gb-scale.png["이 이미지는 2GB/sec 및 4GB/sec의 스케일아웃 성능을 보여줍니다."]

아래 차트는 EC2 인스턴스와 FSxN(Kafka Replication Factor: 3) 간의 성능을 보여줍니다.

image:aws-fsxn-ec2-fsxn-comparition.png["이 이미지는 RF3에서 EC2와 FSxN의 성능을 비교한 것입니다."]

link:kafka-nfs-performance-overview-and-validation-with-aff-on-premises.html["다음: AFF 온-프레미스를 사용한 성능 개요 및 검증."]
