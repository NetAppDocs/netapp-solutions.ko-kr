---
sidebar: sidebar 
permalink: data-analytics/apache-spark-solution-technology.html 
keywords: standalone, apache mesos, hadoop yarn, resilient distributed dataset, rdd, dataframe, hadoop distributed file system, hdfs 
summary: 이 섹션에서는 Apache Spark의 특성 및 구성 요소와 이러한 구성 요소가 이 솔루션에 어떻게 기여하는지 설명합니다. 
---
= 솔루션 기술
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Apache Spark는 HDFS(Hadoop Distributed File System)와 직접 연동되는 Hadoop 애플리케이션을 작성하기 위한 인기 있는 프로그래밍 프레임워크입니다. Spark는 프로덕션을 지원하며 스트리밍 데이터의 처리를 지원하며 MapReduce보다 빠릅니다. Spark는 효율적인 반복을 위해 메모리 내 데이터 캐싱을 구성할 수 있으며 Spark 셸은 데이터를 학습하고 탐색하는 대화형 기능을 제공합니다. Spark를 사용하면 Python, Scala 또는 Java로 애플리케이션을 만들 수 있습니다. SPARK 응용 프로그램은 하나 이상의 작업이 있는 하나 이상의 작업으로 구성됩니다.

모든 Spark 응용 프로그램에는 Spark 드라이버가 있습니다. YARN-Client 모드에서 드라이버는 클라이언트에서 로컬로 실행됩니다. YARN-Cluster 모드에서는 드라이버가 애플리케이션 마스터의 클러스터에서 실행됩니다. 클러스터 모드에서는 클라이언트의 연결이 끊기더라도 애플리케이션이 계속 실행됩니다.

image::apache-spark-image3.png[아파치 스파크 이미지3]

세 가지 클러스터 관리자가 있습니다.

* * 독립 실행형. * 이 매니저는 Spark의 일부이며, 클러스터를 쉽게 설정할 수 있습니다.
* * Apache Mesos. * MapReduce 및 기타 애플리케이션도 실행하는 일반 클러스터 관리자입니다.
* * Hadoop YARN. * Hadoop 3의 리소스 관리자입니다.


탄력적인 분산 데이터 세트(RDD)는 Spark의 주요 구성 요소입니다. RDD는 클러스터의 메모리에 저장된 데이터에서 손실되거나 누락된 데이터를 재생성하고 파일에서 나오거나 프로그래밍 방식으로 생성된 초기 데이터를 저장합니다. RDD는 파일, 메모리에 있는 데이터 또는 다른 RDD에서 생성됩니다. SPARK 프로그래밍은 변환과 동작이라는 두 가지 작업을 수행합니다. 변환은 기존 RDD를 기반으로 새 RDD를 생성합니다. 작업은 RDD에서 값을 반환합니다.

변환 및 작업은 Spark 데이터 집합 및 DataFrames에도 적용됩니다. 데이터 세트는 분산 데이터 모음으로서 Spark SQL의 최적화된 실행 엔진의 이점과 함께 RDD(강력한 타이핑, 람다 함수 사용)의 이점을 제공합니다. 데이터세트는 JVM 객체에서 생성한 다음 기능 변환(맵, 평면 맵, 필터 등)을 사용하여 조작할 수 있습니다. DataFrame 은 명명된 열로 구성된 데이터 집합입니다. 개념적으로 관계형 데이터베이스의 테이블 또는 R/Python의 데이터 프레임에 해당합니다. DataFrames는 정형 데이터 파일, Hive/HBase의 테이블, 사내 또는 클라우드의 외부 데이터베이스 또는 기존 RDD와 같은 다양한 소스에서 구성할 수 있습니다.

스파크 응용 프로그램에는 하나 이상의 스파크 작업이 포함됩니다. 작업은 실행기에서 작업을 실행하고 실행자는 YARN 컨테이너에서 실행됩니다. 각 실행자는 단일 컨테이너에서 실행되며 실행자는 응용 프로그램 수명 내내 존재합니다. 응용 프로그램이 시작된 후 실행자가 수정되고 YARN은 이미 할당된 컨테이너의 크기를 조정하지 않습니다. 집행자는 인메모리 데이터에 대해 작업을 동시에 실행할 수 있습니다.
